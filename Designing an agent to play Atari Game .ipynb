{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Deisigning AI to Play Atari Game\n",
    "\n",
    "### Reinforcement Learning: \n",
    "Basic idea behind reinforcement learning is that an agent will learn from interacting with the environment and receiving rewards for performing actions. The purpose of reinforcement learning is to maximize cumulative rewards by taking suitable actions in a particular situation.\n",
    "\n",
    "RL loop outputs a sequence of state, action and reward:\n",
    "1. In state S0 from the environment\n",
    "2. Takes an action A0\n",
    "3. Transition to new state S1\n",
    "4. Receives some rward R1\n",
    "\n",
    "Gamma, the discount factor is used to discount rewards as the rewards that come sooner are more probable to happen than the long term future rewards. Therefore, gamma is used to penalize long term future rewards.\n",
    "\n",
    "There are two types of tasks: episodic and continuous. I took the episodic approach for my project. Episodic tasks have a starting point and an ending point (a terminal state). Continuous tasks continue forever with no terminal state. \n",
    "\n",
    "There are two ways of learning: Monte Carlo and Temporal Difference (TD) Learning: \n",
    "1. Monte Carlo: cumulative rewards are received at the end of the game then calculate the maximum expected future reward\n",
    "2. TD: estimates the reward at each step \n",
    "\n",
    "**Exploration/Exploitation trade off:**\n",
    "- Exploration: finding more information about the environment\n",
    "- Exploitation: exploiting known information to maximize the reward\n",
    "- Exploitation will make the agent find the route to the largest cumulative rewards but will not know whether better route exists without exploration. \n",
    "- Epsilon: exploration rate\n",
    "\n",
    "**Three Approaches to RL:** \n",
    "1. Value Based\n",
    "2. Policy Based\n",
    "3. Model Based: create a model of the behaviour of the environment\n",
    "\n",
    "**Deep Reinforcement Learning:**\n",
    "Deep neural networks to solve Reinforcement Learning Problems \n",
    "ie) Q-learning and Deep Q-learning\n",
    "\n",
    "\n",
    "<img src=\"https://cdn-media-1.freecodecamp.org/images/1*w5GuxedZ9ivRYqM_MLUxOQ.png\" width=\"400\">\n",
    "\n",
    "#### Q-learning\n",
    "Q-learning is a value-based RL algorithm that is used to find the optimal action-selection policy using a Q function. \n",
    "\n",
    "Q(s,a) : action-value function that returns expected future reward of an action\n",
    "In Q-learning, we build a memory table Q[s,a] to store Q-values for all possible combinations of s and a.\n",
    "\n",
    "<img src=\"https://cdn-media-1.freecodecamp.org/images/1*QeoQEqWYYPs1P8yUwyaJVQ.png\" width=\"200\">\n",
    "\n",
    "1. Initialize Q-values\n",
    "2. Loop through number of episodes until learning is stopped\n",
    "3. Choose an action (a) in the current state s --> implement exploration/exploitation trade-off\n",
    "    a) From Q-table, determine action (a) with maximum Q\n",
    "    b) sample an action \n",
    "4. Observe the outcome state (s') and reward (r) \n",
    "5. Update Q(s,a) table using function Q(s,a)\n",
    "\n",
    "`New_Q = Current_Q + lr * [R + gamma* (highest Q of s') - Current_Q]`\n",
    "\n",
    "\n",
    "#### Deep Q Network (DQN)\n",
    "If the memory and the computation requirement for Q will be too high, switch to Deep Q Network to approximate Q(s,a). DQN takes a state then approximates Q-values for each action based on that state using neural network.\n",
    "\n",
    "My Deep Q Neural Netook takes a stack of four frames as an input and these pass through the network outputing Q values for each possible action in the given state. \n",
    "\n",
    "**Preprocessing:**\n",
    "- Grayscale as colour does not provide any important information, reduce three channels (RGB) to 1\n",
    "- Reduce the size of the frame \n",
    "- Stack four frames together to catch motion \n",
    "\n",
    "**Convolution Networks:** \n",
    "- Processed by three convolution layers to exploit spatial relationships in images\n",
    "- Each convolution layer will use Relu as an activation function\n",
    "- One fully connected layer with Relu action function \n",
    "- One output layer: Q-values of each action\n",
    "\n",
    "**Experience Replay**\n",
    "- Avoid forgetting previous experiences and reduce correlations between experiences\n",
    "- When sequential samples are given to neural netowrk, tends to forget the previous experiences as it overwrites with new experiences. To avoid this, create a replay buffer that stores experiences then sample a small batch to feed into neural network \n",
    "\n",
    "\n",
    "### Open AI \n",
    "\n",
    "- Gym is a toolkit for developing and comparing reinforcement learning algorithms \n",
    "- Has a collection of simulated environments where we can easily implement our RL algorithm \n",
    "- Examples: Classic (Cart-Pole); Atari (Tennis) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from gym import wrappers\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01 # learning rate\n",
    "alpha_decay = 0.01\n",
    "gamma = 0.99 # Discount\n",
    "epsilon = 1.0 # Exploration\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.9999\n",
    "\n",
    "batch_size = 32\n",
    "n_episodes = 50_000\n",
    "update_target_rate = 10_000\n",
    "train_start = 10_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented to neural network model in **Mnih et al. 2015, 'Human-level control through deep reinforcement learning'**:\n",
    "\n",
    "Neural network model design: \n",
    "1. Convolves 32 filters of 8 x 8 with stride 4\n",
    "2. Convolves 64 filters of 4 x 4 with stride 2\n",
    "3. Convolves 54 filters of 3 x 3 with stride 1\n",
    "4. Fully connected layer of 512 rectifier units\n",
    "5. Fully connected linear layer wih a single output for each validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(state_shape, action_shape):\n",
    "    # Function to build a neural network model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=state_shape))\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(action_shape))\n",
    "    model.compile(loss=huber_loss, optimizer=Adam(lr=alpha, decay=alpha_decay))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Neural Network is trained using the stochastic gradient descent optimization algorithm and weights are updated using the backpropagation of error algorithm.\n",
    "\n",
    "**Huber Loss function**\n",
    "Acts like the mean squared error when the error is small, but like the mean absolute error when the error is large "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(a, b, in_keras=True):\n",
    "    error = a - b\n",
    "    quadratic_term = error*error / 2\n",
    "    linear_term = abs(error) - 1/2\n",
    "    use_linear_term = (abs(error) > 1.0)\n",
    "    if in_keras:\n",
    "        # Keras won't let us multiply floats by booleans, so we explicitly cast the booleans to floats\n",
    "        use_linear_term = K.cast(use_linear_term, 'float32')\n",
    "    return use_linear_term * linear_term + (1-use_linear_term) * quadratic_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(state):\n",
    "    # Preprocessing frame \n",
    "    processed_observe = np.uint8(resize(rgb2gray(state), (84, 84), mode='constant') * 255)\n",
    "    return processed_observe\n",
    "\n",
    "def reshape_frames(frames, state_shape):\n",
    "    # Reshaping frame \n",
    "    return frames.reshape(1, state_shape[0], state_shape[1], state_shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other functions that will be constantly used for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remember(state, action, reward, next_state, done):\n",
    "    memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "def choose_action(state, epsilon):\n",
    "    return env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(model.predict(state))\n",
    "\n",
    "def get_epsilon(epsilon):\n",
    "    return max(epsilon_min, epsilon*epsilon_decay)\n",
    "\n",
    "def train_replay(batch_size, epsilon):\n",
    "    x_batch, y_batch = [], []\n",
    "    minibatch = random.sample(memory, min(len(memory), batch_size))\n",
    "\n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "        y_target = model.predict(state)\n",
    "        y_target[0][action] = reward if done else reward + gamma * np.max(model.predict(next_state)[0])\n",
    "        x_batch.append(state[0])\n",
    "        y_batch.append(y_target[0])\n",
    "\n",
    "    model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting an enviroment and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 1,687,206\n",
      "Trainable params: 1,687,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "\n",
    "render = True\n",
    "load_model = False\n",
    "\n",
    "# Environment settings\n",
    "state_shape = (84, 84, 4)\n",
    "action_shape = env.action_space.n\n",
    "\n",
    "# Memory\n",
    "memory = deque(maxlen=1_000_000)\n",
    "\n",
    "# Build models\n",
    "model = build_model(state_shape, action_shape)\n",
    "\n",
    "# loads weights  \n",
    "if load_model:\n",
    "    model.load_weights(\"dqn12_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = deque(maxlen=100)\n",
    "global_step = 0\n",
    "\n",
    "for e in range(n_episodes):\n",
    "    done = False\n",
    "    step = 0\n",
    "    score = 0\n",
    "    imgs = list()\n",
    "\n",
    "    state = env.reset()\n",
    "    frame = preprocess_image(state)\n",
    "    # 4 frames\n",
    "    frames = reshape_frames(np.stack((frame, frame, frame, frame), axis=2), state_shape)\n",
    "\n",
    "    while not done:\n",
    "        if render:\n",
    "            img = env.render(mode='rgb_array')\n",
    "            imgs.append(img)\n",
    "        global_step += 1\n",
    "        step += 1\n",
    "\n",
    "        action = choose_action(frames, epsilon)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        frame = reshape_frames(preprocess_image(state), (84, 84, 1))\n",
    "        next_frames = np.append(frame, frames[:, :, :, :3], axis=3)\n",
    "\n",
    "        remember(frames, action, reward, next_frames, done)\n",
    "\n",
    "        frame = next_frames\n",
    "        reward = np.clip(reward, -1., 1.)\n",
    "        score += reward\n",
    "\n",
    "        if done:\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\", len(memory), \"  epsilon:\", epsilon,\n",
    "                  \"  global_step:\", global_step, \"   step:\", step)\n",
    "            step = 0\n",
    "            \n",
    "    imgs= np.asarray(imgs)\n",
    "    np.savez_compressed(f'./dqn12/dqn1_v2_Episode{e}', imgs)            \n",
    "\n",
    "    train_replay(batch_size, epsilon)\n",
    "    \n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "    scores.append(score)\n",
    "    mean_score = np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saves model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('dqn12_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(loaded_arr):\n",
    "    plt.figure(figsize = (20,10))\n",
    "    img = plt.imshow(loaded_arr[0]) # only call this once\n",
    "    plt.axis('off')\n",
    "\n",
    "    for i in loaded_arr:\n",
    "        img.set_data(i) # just update the data\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Episode 0\n",
    "\n",
    "This is the first Pong Game my agent plays (green on is my agent). It is only making random motions that are irrelevant to the motion of the ball. For example, when the ball is moving down, my agent is moving up. \n",
    "\n",
    "**Score** 21: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads a npz file for episode 0\n",
    "episode0 = np.load('./dqn1/dqn1_v2_Episode0.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plays frames saved for episode 0 in a loop \n",
    "#need to unhash the line below to see a video of episode 0\n",
    "#play_episode(episode0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Episode 510\n",
    "\n",
    "After 510 games, my agent learned to be a better player. It learned to chase after a ball to get rewards (score a point) and also to hit the ball when it is nearby. The agent managed to score 6 points. \n",
    "\n",
    "**Score** 21: 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_t = np.load('./dqn1/dqn1_v2_Episode510.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to unhash the line below to see a video of episode 510\n",
    "#play_episode(episode_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "\n",
    "With limits of computation power and time, the agent was not able to learn to win the game but learned to hit the ball and score 6 points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future areas of interest\n",
    "\n",
    "1. Improve DQN Model \n",
    "    - Double DQN\n",
    "    - Dueling Networks\n",
    "    - Prioritized Experience Replay\n",
    "    - Change Parameters (epsilon, epsilon decay, etc.) \n",
    "\n",
    "\n",
    "2. Research Different Approaches ie. A3C\n",
    "\n",
    "3. Different applications to RL \n",
    "    - Traffic Light Control\n",
    "    - Robotics/ Self-driving\n",
    "    - Personalized Recommendations\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "1. https://towardsdatascience.com/welcome-to-deep-reinforcement-learning-part-1-dqn-c3cab4d41b6b\n",
    "2. https://towardsdatascience.com/simple-reinforcement-learning-q-learning-fcddc4b6fe56\n",
    "3. https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/\n",
    "4. https://www.freecodecamp.org/news/an-introduction-to-reinforcement-learning-4339519de419/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
